{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_homework_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ_mlf5cqln8"
      },
      "source": [
        "**NLP Homework 2** *by Kharkhanova Ekaterina*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n-FPTAoMWY_"
      },
      "source": [
        "Корпусом для этого задания послужил набор текстов развлекательного характера из твиттера (объем 230 слов). Для оценки был выбран именно такой текст, т к в нем есть следующие сложные для POS теггинга моменты: идентификация англицизмов *(би лайк, тотал блэк),* ряд видоизменненных русских слов *(крышак, выспанный)*, абревиатуры *(СДВГ, СНГ)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1w13r5GjgSi"
      },
      "source": [
        "Для разметки был выбран тегсет, используемый в spacy с незначительными изменениями (ADP -> PREP, CCONJ -> CONJ) сделанными для упрощения, т к стоит задача только определить частеречную принадлежность слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXH8GMNnpI0G",
        "outputId": "d44868cf-a5c5-4c4b-d6e2-95e190e36b24"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"корпус.csv\")\n",
        "words = df[\"токены\"].tolist() # текст отдельными словами в список (для pymorphy2)\n",
        "# и текст строкой с разделением на предложения, но с трудностями для парсинга, т к это твиттер и там нет большей части знаков препинания\n",
        "text = \"мне нравится смотреть как у нас у всех осенью подтекает крышак. знаете такое единение. ощущение что ты не один в этой ловушке. а когда у тебя и так депрессия еще осенняя хандра, так это вообще комбо. быть околотворческим человеком это каждый раз когда переживаешь сильную негативную эмоцию тыкать в мозг палкой со словами ну давай сделай что-нибудь прикольное из этого. девяносто процентов тусовок в последнее время би лайк. куда ни глянь все на фрилансе и рубят бабки а я будто единственная тупая картошина. давайте признаем что нет ничего сексуальнее чем когда человек весь такой тотал блэк черная футболка черная кожанка черные джинсы черная обувь а под этой обувью самые дурацкие в мире носки. они показывают что это ок носить одну куртку три года. умные наушники с шумоподавлением. а может лучше шумные наушники с умоподавлением? у студентов и учеников снг три пути синдром отличника сдвг и апатия с выгоранием. слизеринцы би лайк. слизеринцы на первом месте в соревновании за кубок школы, но учитывая последние события. давайте честно несмотря на то что фуфелшмерц травмированный злодей он тот отец о котором мечтает большая часть страны. приказываю всем любить фуфелшмерца. это я выспанный настраиваюсь на продуктивную рабочую неделю. ты в шестнадцать лет: йоууу тусим до утра потом поспим два часа и продолжим. ты сейчас: часок посидели и хватит дома столько всего прикольного не посмотрено и не съедено да и вообще как-то спатеньки уже хочется.\"\n",
        "print(words)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['мне', 'нравится', 'смотреть', 'как', 'у', 'нас', 'у', 'всех', 'осенью', 'подтекает', 'крышак', 'знаете', 'такое', 'единение', 'ощущение', 'что', 'ты', 'не', 'один', 'в', 'этой', 'ловушке', 'а', 'когда', 'у', 'тебя', 'и', 'так', 'депрессия', 'еще', 'осенняя', 'хандра', 'так', 'это', 'вообще', 'комбо', 'быть', 'околотворческим', 'человеком', 'это', 'каждый', 'раз', 'когда', 'переживаешь', 'сильную', 'негативную', 'эмоцию', 'тыкать', 'в', 'мозг', 'палкой', 'со', 'словами', 'ну', 'давай', 'сделай', 'что-нибудь', 'прикольное', 'из', 'этого', 'девяносто', 'процентов', 'тусовок', 'в', 'последнее', 'время', 'би', 'лайк', 'куда', 'ни', 'глянь', 'все', 'на', 'фрилансе', 'и', 'рубят', 'бабки', 'а', 'я', 'будто', 'единственная', 'тупая', 'картошина', 'давайте', 'признаем', 'что', 'нет', 'ничего', 'сексуальнее', 'чем', 'когда', 'человек', 'весь', 'такой', 'тотал', 'блэк', 'черная', 'футболка', 'черная', 'кожанка', 'черные', 'джинсы', 'черная', 'обувь', 'а', 'под', 'этой', 'обувью', 'самые', 'дурацкие', 'в', 'мире', 'носки', 'они', 'показывают', 'что', 'это', 'ок', 'носить', 'одну', 'куртку', 'три', 'года', 'умные', 'наушники', 'с', 'шумоподавлением', 'а', 'может', 'лучше', 'шумные', 'наушники', 'с', 'умоподавлением', 'у', 'студентов', 'и', 'учеников', 'снг', 'три', 'пути', 'синдром', 'отличника', 'сдвг', 'и', 'апатия', 'с', 'выгоранием', 'слизеринцы', 'би', 'лайк', 'слизеринцы', 'на', 'первом', 'месте', 'в', 'соревновании', 'за', 'кубок', 'школы', 'но', 'учитывая', 'последние', 'события', 'давайте', 'честно', 'несмотря', 'на', 'то', 'что', 'фуфелшмерц', 'травмированный', 'злодей', 'он', 'тот', 'отец', 'о', 'котором', 'мечтает', 'большая', 'часть', 'страны', 'приказываю', 'всем', 'любить', 'фуфелшмерца', 'это', 'я', 'выспанный', 'настраиваюсь', 'на', 'продуктивную', 'рабочую', 'неделю', 'ты', 'в', 'шестнадцать', 'лет', 'йоууу', 'тусим', 'до', 'утра', 'потом', 'поспим', 'два', 'часа', 'и', 'продолжим', 'ты', 'сейчас', 'часок', 'посидели', 'и', 'хватит', 'дома', 'столько', 'всего', 'прикольного', 'не', 'посмотрено', 'и', 'не', 'съедено', 'да', 'и', 'вообще', 'как-то', 'спатеньки', 'уже', 'хочется']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9xdKVr_qjj-"
      },
      "source": [
        "Прогоняем текст через три POS теггера"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ELb2wUaysw"
      },
      "source": [
        "**Pymorphy2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH4grA7ZSEnv",
        "outputId": "a327b1ea-b907-491f-afa1-b2f7d6642cbe"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 16.9 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXWnqkLyTnbW",
        "outputId": "d372731e-8791-4902-fb75-bdbafcce7bbe"
      },
      "source": [
        "import pymorphy2\n",
        "\n",
        "tags_pymorphy2 = []\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "for word in words:\n",
        "  tag_variants = []\n",
        "  for i in morph.parse(word):\n",
        "    if i.tag.POS not in tag_variants:\n",
        "      tag_variants.append(str(i.tag.POS))\n",
        "\n",
        "  #tags_pymorphy2.append(\n",
        "  tags_pymorphy2.append(tag_variants)\n",
        "\n",
        "print(tags_pymorphy2)\n",
        "print(len(tags_pymorphy2))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['NPRO'], ['VERB'], ['INFN'], ['CONJ', 'ADVB', 'PRCL'], ['PREP', 'INTJ'], ['NPRO'], ['PREP', 'INTJ'], ['ADJF'], ['NOUN', 'ADVB'], ['VERB'], ['NOUN'], ['VERB', 'CONJ'], ['ADJF'], ['NOUN'], ['NOUN'], ['CONJ', 'PRCL', 'NPRO', 'ADVB'], ['NPRO'], ['PRCL'], ['ADJF'], ['PREP', 'NOUN'], ['ADJF'], ['NOUN'], ['CONJ', 'INTJ', 'PRCL'], ['CONJ', 'ADVB'], ['PREP', 'INTJ'], ['NPRO'], ['CONJ', 'PRCL', 'INTJ', 'NOUN'], ['CONJ', 'ADVB', 'PRCL'], ['NOUN'], ['ADVB', 'PRCL'], ['ADJF'], ['NOUN'], ['CONJ', 'ADVB', 'PRCL'], ['PRCL', 'NPRO', 'ADJF'], ['ADVB', 'CONJ'], ['NOUN'], ['INFN'], ['ADJF'], ['NOUN'], ['PRCL', 'NPRO', 'ADJF'], ['ADJF'], ['NOUN', 'ADVB', 'CONJ'], ['CONJ', 'ADVB'], ['VERB'], ['ADJF'], ['ADJF'], ['NOUN'], ['INFN'], ['PREP', 'NOUN'], ['NOUN'], ['NOUN'], ['PREP', 'NOUN'], ['NOUN'], ['PRCL', 'INTJ'], ['VERB'], ['VERB'], ['NPRO'], ['ADJF'], ['PREP', 'NOUN'], ['NPRO', 'ADJF'], ['NUMR'], ['NOUN'], ['NOUN'], ['PREP', 'NOUN'], ['ADJF'], ['NOUN'], ['None'], ['NOUN'], ['CONJ', 'PRCL', 'NOUN', 'ADVB'], ['PRCL', 'CONJ'], ['VERB'], ['PRCL', 'ADJF'], ['PREP', 'PRCL', 'INTJ'], ['NOUN'], ['CONJ', 'PRCL', 'INTJ', 'NOUN'], ['VERB'], ['NOUN'], ['CONJ', 'INTJ', 'PRCL'], ['NPRO'], ['CONJ', 'PRCL', 'INTJ'], ['ADJF'], ['ADJF'], ['NOUN'], ['VERB'], ['VERB'], ['CONJ', 'PRCL', 'NPRO', 'ADVB'], ['PRED', 'PRCL', 'INTJ'], ['ADVB', 'PRCL', 'NPRO'], ['COMP'], ['CONJ', 'NPRO'], ['CONJ', 'ADVB'], ['NOUN'], ['ADJF', 'NOUN', 'VERB'], ['ADJF'], ['ADJS', 'NOUN', 'VERB'], ['NOUN', 'ADVB'], ['ADJF'], ['NOUN'], ['ADJF'], ['NOUN'], ['ADJF', 'NOUN'], ['NOUN'], ['ADJF'], ['NOUN'], ['CONJ', 'INTJ', 'PRCL'], ['PREP', 'NOUN'], ['ADJF'], ['NOUN'], ['ADJF'], ['ADJF'], ['PREP', 'NOUN'], ['NOUN'], ['NOUN', 'ADJF', 'ADJS'], ['NPRO'], ['VERB'], ['CONJ', 'PRCL', 'NPRO', 'ADVB'], ['PRCL', 'NPRO', 'ADJF'], ['PREP', 'PRCL'], ['INFN'], ['ADJF'], ['NOUN'], ['NUMR', 'VERB'], ['NOUN'], ['ADJF'], ['NOUN'], ['PREP', 'NOUN', 'PRCL'], ['NOUN'], ['CONJ', 'INTJ', 'PRCL'], ['VERB', 'CONJ'], ['COMP', 'PRCL'], ['ADJF'], ['NOUN'], ['PREP', 'NOUN', 'PRCL'], ['NOUN'], ['PREP', 'INTJ'], ['NOUN'], ['CONJ', 'PRCL', 'INTJ', 'NOUN'], ['NOUN'], ['NOUN'], ['NUMR', 'VERB'], ['NOUN'], ['NOUN'], ['NOUN'], ['NOUN', 'ADJS', 'VERB', 'ADVB'], ['CONJ', 'PRCL', 'INTJ', 'NOUN'], ['NOUN'], ['PREP', 'NOUN', 'PRCL'], ['NOUN'], ['NOUN'], ['None'], ['NOUN'], ['NOUN'], ['PREP', 'PRCL', 'INTJ'], ['ADJF', 'NOUN'], ['NOUN'], ['PREP', 'NOUN'], ['NOUN'], ['PREP'], ['NOUN'], ['NOUN'], ['CONJ', 'INTJ'], ['GRND'], ['ADJF'], ['NOUN'], ['VERB'], ['ADVB', 'ADJS'], ['PREP'], ['PREP', 'PRCL', 'INTJ'], ['CONJ', 'PRCL', 'ADJF'], ['CONJ', 'PRCL', 'NPRO', 'ADVB'], ['NOUN'], ['ADJF', 'PRTF'], ['NOUN'], ['NPRO'], ['ADJF'], ['NOUN'], ['PREP', 'INTJ', 'NOUN'], ['ADJF', 'NOUN'], ['VERB'], ['ADJF'], ['NOUN'], ['NOUN'], ['VERB'], ['ADJF'], ['INFN'], ['NOUN'], ['PRCL', 'NPRO', 'ADJF'], ['NPRO'], ['PRTF', 'ADJF', 'NOUN'], ['VERB'], ['PREP', 'PRCL', 'INTJ'], ['ADJF'], ['ADJF'], ['NOUN'], ['NPRO'], ['PREP', 'NOUN'], ['NUMR'], ['NOUN'], ['NOUN'], ['NOUN', 'VERB', 'PRTS'], ['PREP'], ['NOUN'], ['ADVB', 'NOUN'], ['VERB'], ['NUMR'], ['NOUN'], ['CONJ', 'PRCL', 'INTJ', 'NOUN'], ['VERB'], ['NPRO'], ['ADVB'], ['NOUN'], ['VERB'], ['CONJ', 'PRCL', 'INTJ', 'NOUN'], ['VERB'], ['NOUN', 'ADVB'], ['ADVB', 'CONJ', 'NUMR'], ['ADJF', 'PRCL', 'ADVB'], ['ADJF'], ['PRCL'], ['PRTS'], ['CONJ', 'PRCL', 'INTJ', 'NOUN'], ['PRCL'], ['PRTS'], ['PRCL', 'CONJ', 'INTJ'], ['CONJ', 'PRCL', 'INTJ', 'NOUN'], ['ADVB', 'CONJ'], ['ADVB'], ['NOUN', 'ADJS', 'ADVB'], ['ADVB', 'PRCL', 'NOUN', 'COMP'], ['VERB']]\n",
            "230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VY-ZKGjQwax"
      },
      "source": [
        "**Spacy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKoE2RFrSImo",
        "outputId": "ab69fab3-0ebf-4b51-cf28-4d4c5758aa9e"
      },
      "source": [
        "!pip uninstall spacy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: spacy 2.2.4\n",
            "Uninstalling spacy-2.2.4:\n",
            "  Would remove:\n",
            "    /usr/local/bin/spacy\n",
            "    /usr/local/lib/python3.7/dist-packages/bin/*\n",
            "    /usr/local/lib/python3.7/dist-packages/spacy-2.2.4.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/spacy/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/bin/theano_cache.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled spacy-2.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qicCBdOvSU-7",
        "outputId": "ba545c51-9154-4f12-f23e-f49c5c317e46"
      },
      "source": [
        "! pip install spacy==3.0.0 -q\n",
        "! python -m spacy download ru_core_news_sm -q"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 12.7 MB 230 kB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 623 kB 70.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 38.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 456 kB 47.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 17.9 MB 169 kB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGopZsGyVaqE",
        "outputId": "4f42e134-6400-4577-e3ef-e01670a98abe"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('ru_core_news_sm')\n",
        "doc = nlp(' '.join(words))\n",
        "\n",
        "tags_spacy = []\n",
        "for word in doc:\n",
        "  if word.pos_ != 'PUNCT':\n",
        "    #print(word)\n",
        "    tags_spacy.append(word.pos_)\n",
        "    \n",
        " # spacy отдельно токенизирует \"-то\", поэтому для\n",
        "                       # дальнейшего подсчета accuracy надо убрать эти элементы\n",
        "\n",
        "# здесь нас волнует как теггер разметил именно первую часть слова\n",
        "tags_spacy.pop(words.index('что-нибудь')+1)\n",
        "tags_spacy.pop(words.index('что-нибудь')+1)\n",
        "tags_spacy.pop(words.index('как-то')+1)\n",
        "tags_spacy.pop(words.index('как-то')+1)\n",
        "print(tags_spacy)\n",
        "print(len(tags_spacy))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRON', 'VERB', 'VERB', 'SCONJ', 'ADP', 'PRON', 'ADP', 'DET', 'NOUN', 'VERB', 'NOUN', 'VERB', 'DET', 'NOUN', 'NOUN', 'SCONJ', 'PRON', 'PART', 'NUM', 'ADP', 'DET', 'NOUN', 'CCONJ', 'SCONJ', 'ADP', 'PRON', 'CCONJ', 'ADV', 'NOUN', 'ADV', 'ADJ', 'NOUN', 'ADV', 'PRON', 'ADV', 'NOUN', 'AUX', 'ADJ', 'NOUN', 'PRON', 'DET', 'NOUN', 'SCONJ', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'PART', 'VERB', 'VERB', 'PRON', 'VERB', 'ADP', 'PRON', 'ADV', 'NOUN', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'ADV', 'PART', 'NOUN', 'PRON', 'ADP', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'CCONJ', 'PRON', 'PART', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', 'PRON', 'VERB', 'PRON', 'ADV', 'SCONJ', 'SCONJ', 'NOUN', 'DET', 'DET', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'CCONJ', 'ADP', 'DET', 'NOUN', 'ADJ', 'ADJ', 'ADP', 'NOUN', 'NOUN', 'PRON', 'VERB', 'SCONJ', 'PRON', 'ADP', 'VERB', 'NUM', 'NOUN', 'NUM', 'NOUN', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'NUM', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'CCONJ', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'CCONJ', 'VERB', 'ADJ', 'NOUN', 'VERB', 'ADV', 'ADV', 'ADP', 'PRON', 'SCONJ', 'NOUN', 'ADJ', 'NOUN', 'PRON', 'DET', 'NOUN', 'ADP', 'PRON', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'PRON', 'VERB', 'NOUN', 'PRON', 'PRON', 'ADJ', 'VERB', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'PRON', 'ADP', 'NUM', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'ADV', 'VERB', 'NUM', 'NOUN', 'CCONJ', 'VERB', 'PRON', 'ADV', 'NOUN', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'ADV', 'PRON', 'NOUN', 'PART', 'VERB', 'CCONJ', 'PART', 'VERB', 'CCONJ', 'CCONJ', 'ADV', 'ADV', 'NOUN', 'ADV', 'VERB']\n",
            "230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88XRxNUd2zOD"
      },
      "source": [
        "**Natasha**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtlo2_xtk2zm",
        "outputId": "d8f52e95-4e5f-4654-94d8-bc2d68d743e4"
      },
      "source": [
        "!pip install natasha"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting natasha\n",
            "  Downloading natasha-1.4.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.4 MB 32 kB/s \n",
            "\u001b[?25hCollecting razdel>=0.5.0\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting yargy>=0.14.0\n",
            "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 128 kB/s \n",
            "\u001b[?25hCollecting ipymarkup>=0.8.0\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.9.1)\n",
            "Collecting navec>=0.9.0\n",
            "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Collecting slovnet>=0.3.0\n",
            "  Downloading slovnet-0.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 139 kB/s \n",
            "\u001b[?25hCollecting intervaltree>=3\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec>=0.9.0->natasha) (1.19.5)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Building wheels for collected packages: intervaltree\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26119 sha256=fb83552ce4b29c20a6fdb256462db0921ea0ec3b0719b7d44e067fb74d68fce7\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/85/bd/1001cbb46dcfb71c2001cd7401c6fb250392f22a81ce3722f7\n",
            "Successfully built intervaltree\n",
            "Installing collected packages: razdel, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "  Attempting uninstall: intervaltree\n",
            "    Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "Successfully installed intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 razdel-0.5.0 slovnet-0.5.0 yargy-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb8HHCErl9B6"
      },
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    \n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    \n",
        "    Doc\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGRVK8QjmKPc",
        "outputId": "c752cd21-e4c8-490c-ab56-3b2beb9ac706"
      },
      "source": [
        "segmenter = Segmenter()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "doc = Doc(text)\n",
        "\n",
        "doc.segment(segmenter)\n",
        "doc.tag_morph(morph_tagger)\n",
        "\n",
        "# сейчас будет извлечение тегов очень странным способом\n",
        "\n",
        "new = pd.DataFrame(doc)\n",
        "strange_object = new.iloc[1].tolist()\n",
        "tags_natasha = []\n",
        "for i in strange_object:\n",
        "  for part in i:\n",
        "    for tag in part:\n",
        "      if isinstance(tag,str):\n",
        "        if tag.isupper():\n",
        "          if tag != 'PUNCT':\n",
        "            tags_natasha.append(tag)\n",
        "\n",
        " #проверяем, что все 230 нужных тегов на месте     \n",
        "print(tags_natasha)\n",
        "print(len(tags_natasha))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRON', 'VERB', 'VERB', 'SCONJ', 'ADP', 'PRON', 'ADP', 'PRON', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'DET', 'NOUN', 'PART', 'SCONJ', 'PRON', 'PART', 'DET', 'ADP', 'DET', 'NOUN', 'CCONJ', 'SCONJ', 'ADP', 'PRON', 'PART', 'ADV', 'NOUN', 'ADV', 'ADJ', 'NOUN', 'ADV', 'PRON', 'ADV', 'NOUN', 'AUX', 'ADJ', 'NOUN', 'PRON', 'DET', 'NOUN', 'SCONJ', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'ADP', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'PART', 'PART', 'VERB', 'PRON', 'VERB', 'ADP', 'PRON', 'NUM', 'NOUN', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'X', 'NOUN', 'ADV', 'PART', 'VERB', 'PRON', 'ADP', 'NOUN', 'CCONJ', 'VERB', 'NOUN', 'CCONJ', 'PRON', 'PART', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', 'SCONJ', 'VERB', 'PRON', 'NOUN', 'PRON', 'SCONJ', 'NOUN', 'DET', 'DET', 'NOUN', 'PART', 'ADJ', 'NOUN', 'ADJ', 'VERB', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'CCONJ', 'ADP', 'DET', 'NOUN', 'ADJ', 'ADJ', 'ADP', 'NOUN', 'NOUN', 'PRON', 'VERB', 'SCONJ', 'PRON', 'NOUN', 'VERB', 'NUM', 'NOUN', 'NUM', 'NOUN', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'VERB', 'ADV', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'CCONJ', 'NOUN', 'NOUN', 'NUM', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'CCONJ', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'X', 'NOUN', 'VERB', 'ADP', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'CCONJ', 'VERB', 'ADJ', 'NOUN', 'VERB', 'ADV', 'ADV', 'ADP', 'PRON', 'SCONJ', 'VERB', 'ADJ', 'NOUN', 'PRON', 'DET', 'NOUN', 'ADP', 'PRON', 'VERB', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'PRON', 'VERB', 'NOUN', 'PRON', 'PRON', 'VERB', 'VERB', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'PRON', 'ADP', 'NUM', 'NOUN', 'ADJ', 'VERB', 'ADP', 'NOUN', 'ADV', 'VERB', 'NUM', 'NOUN', 'CCONJ', 'VERB', 'PRON', 'ADV', 'NOUN', 'VERB', 'CCONJ', 'VERB', 'NOUN', 'ADV', 'PRON', 'NOUN', 'PART', 'VERB', 'CCONJ', 'PART', 'VERB', 'PART', 'CCONJ', 'ADV', 'ADV', 'VERB', 'ADV', 'VERB']\n",
            "230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Z4oY6LlrMp"
      },
      "source": [
        "Создаем функцию-конвертер для тегов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8NXBkineP5Q"
      },
      "source": [
        "def unify(tagset):\n",
        "  for i, tag in enumerate(tagset):\n",
        "    if tag == 'ADP':\n",
        "      tagset[i] = 'PREP'\n",
        "    elif tag == 'CCONJ':\n",
        "      tagset[i] = 'CONJ'\n",
        "    elif tag == 'ADJF' or tag == 'ADJS' or tag == 'COMP':\n",
        "      tagset[i] = 'ADJ'\n",
        "    elif tag in ('INFN', 'PRTF', 'PRTS', 'GRND'):\n",
        "      tagset[i] = 'VERB'\n",
        "    elif tag == 'NUMR':\n",
        "      tagset[i] = 'NUM'\n",
        "    elif tag == 'ADVB':\n",
        "      tagset[i] = 'ADV'\n",
        "    elif tag == 'NPRO':\n",
        "      tagset[i] = 'PRON'\n",
        "    elif tag == 'PRED':\n",
        "      tagset[i] = 'ADV'\n",
        "    elif tag == 'PRCL':\n",
        "      tagset[i] = 'PART'\n",
        "  return tagset\n",
        "\n",
        "tags_natasha = unify(tags_natasha)  \n",
        "tags_spacy = unify(tags_spacy)\n",
        "tags_pymorphy2 = [unify(i) for i in tags_pymorphy2]\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syIPQ_JHeKri"
      },
      "source": [
        "Проверяем accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvQbXl0Jxv5v",
        "outputId": "1125d949-2453-4c3d-ea48-7863887a72b1"
      },
      "source": [
        "df = pd.read_csv(\"gold_standard.csv\")\n",
        "gold_standard = df[\"POS-теги\"].tolist()\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_true = gold_standard\n",
        "\n",
        "# natasha\n",
        "y_pred = tags_natasha\n",
        "acc_natasha = accuracy_score(y_true, y_pred)\n",
        "print(acc_natasha)\n",
        "\n",
        "# spacy\n",
        "y_pred = tags_spacy\n",
        "acc_spacy = accuracy_score(y_true, y_pred)\n",
        "print(acc_spacy)\n",
        "\n",
        "# pymorphy2\n",
        "right_answers = 0\n",
        "for i, tag in enumerate(gold_standard):\n",
        "  if tag in tags_pymorphy2[i]:\n",
        "    right_answers += 1\n",
        "acc_pymorphy2 = right_answers / len(gold_standard)\n",
        "print(acc_pymorphy2)\n",
        "  \n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7913043478260869\n",
            "0.8043478260869565\n",
            "0.8826086956521739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0FXsmeK1PjZ"
      },
      "source": [
        "Лучший результат показал **Pymorphy2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTV4aJ2OD0ph"
      },
      "source": [
        "Попробуем с его помощью выделить здесь 3 типа n-грамм:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E9flZitEAxp",
        "outputId": "c8187803-f5de-445e-ec4c-04550db55812"
      },
      "source": [
        "# прилагательное + существительное\n",
        "\n",
        "chunk_1 = []\n",
        "for i, set in enumerate(tags_pymorphy2):\n",
        "  if 'ADJ' in set:\n",
        "    if 'NOUN' in tags_pymorphy2[i + 1]:\n",
        "      collocation = words[i] + ' ' + words[i + 1]\n",
        "      chunk_1.append(collocation)\n",
        "\n",
        "print(chunk_1)\n",
        "\n",
        "# наречие + глагол\n",
        "\n",
        "chunk_2 = []\n",
        "for i, set in enumerate(tags_pymorphy2):\n",
        "  if 'ADV' in set:\n",
        "    if 'VERB' in tags_pymorphy2[i + 1]:\n",
        "      collocation = words[i] + ' ' + words[i + 1]\n",
        "      chunk_2.append(collocation)\n",
        "\n",
        "print(chunk_2)\n",
        "\n",
        "# предлог \"с\" + существительное\n",
        "\n",
        "chunk_3 = []\n",
        "for i, word in enumerate(words):\n",
        "  if word == 'с':\n",
        "    if 'NOUN' in tags_pymorphy2[i + 1]:\n",
        "      collocation = words[i] + ' ' + words[i + 1]\n",
        "      chunk_3.append(collocation)\n",
        "\n",
        "print(chunk_3)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['всех осенью', 'такое единение', 'один в', 'этой ловушке', 'осенняя хандра', 'околотворческим человеком', 'каждый раз', 'негативную эмоцию', 'прикольное из', 'последнее время', 'тупая картошина', 'такой тотал', 'тотал блэк', 'черная футболка', 'черная кожанка', 'черные джинсы', 'черная обувь', 'этой обувью', 'дурацкие в', 'одну куртку', 'умные наушники', 'шумные наушники', 'сдвг и', 'первом месте', 'последние события', 'травмированный злодей', 'тот отец', 'большая часть', 'рабочую неделю', 'спатеньки уже']\n",
            "['осенью подтекает', 'когда переживаешь', 'потом поспим', 'уже хочется']\n",
            "['с шумоподавлением', 'с умоподавлением', 'с выгоранием']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsKB5GLdIElL"
      },
      "source": [
        "Работает, но в первом теги не все были верно выставлены библиотекой, поэтому есть неподходящие примеры"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0KYSD8D_TAq"
      },
      "source": [
        "Идеи по улучшению предыдущей домашки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0fgvGX1_dfI"
      },
      "source": [
        "Три паттерна, которые потенциально улучшили бы работу модели из прошлой домашки:\n",
        "1) \"не\" + прилагательное (в отзывах часто нарушена орфография из-за чего, например, слово \"неплохой\" может делиться на два токена и из-за наличия слова \"плохой\" отзыв будет маркироваться как отрицательный; эта закономерность может работать в обе стороны)\n",
        "2) прилагательное + существительное (в этом паттерне существительные, попавшие в выборку могут стать явно эмоционально маркированы за счет прилагательных, т е производительность попадет не только, например, в положительный список, но и в отрицательный, если она низкая)\n",
        "3) наречие + глагол (по аналогии с предыдущим пунктом глаголы могут приобрести более явную эмоциональную окраску)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cugbIktqIikx"
      },
      "source": [
        "Остальная часть последнего пункта задания находится в файле \"NLP_homework_1_upd.ipynb\""
      ]
    }
  ]
}